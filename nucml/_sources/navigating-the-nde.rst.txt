.. _navigating-the-nde-label:


.. Note::

    Documentation in progress. 


Loading Data
============

Identifying, parsing, and formatting all nuclear data sources can be a tedious time-consuming job. NucML contains a variety of utilities 
that make it easy to download and load the latest versions of the EXFOR, RIPL, ENDF, and AME libraries easily. To convert these into ML-friendly datasets, 
parsing utilities are available to read library-native formats, restructure the information, and store the resulting data structure into single 
easy-to-use files.

.. toctree::
    :maxdepth: 2

    notebooks/1_Loading_AME_Datasets
    notebooks/2_Loading_EXFOR_Datasets
    notebooks/1_Loading_ENSDF_RIPL_Datasets
    notebooks/0_Loading_and_Plotting_Evaluations

Exploratory Data Analysis
=========================

Thoroughly exploring and analyzing your data before modeling is very important. Both EXFOR and XUNDL/RIPL are very unconventional datasets compared 
to traditional structured data. Care must be taken to understand the strengths but also limitations.

.. toctree::
    :maxdepth: 1

    notebooks/2_EDA_AME
    notebooks/3_EDA_EXFOR
    notebooks/2_EDA_RIPL


Modeling Data
=============



In the evaluation phase of the traditional NDE pipeline, relevant data is used to guide physics-based model calculations which result in best estimates, 
dependent on data availability, of mean values including uncertainties and covariances. These values can then form part of one or more regional libraries (i.e., ENDF). 
As previously mentioned, the ML-NDE pipeline instead makes use of trained ML models to create reaction cross-section data and therefore to generate ML-based libraries. 

The NucML Model utilities provide various python script examples to train various ML algorithms including scikit-learn models (i.e. K-nearest-neighbors, 
Decision Trees), Gradient Boosting Machines, and Neural Networks. It is built around a strict ML management philosophy by keeping track of model 
hyperparameters and resulting performance metrics for the supported models. Other ML management tools like Comet ML and Weights and Biases can be 
configured and used using user-provided credentials. It is the goal of NucML to first and foremost provide researchers the framework and tools to create, 
train, and analyze their models rather than providing a set of optimized algorithms. 

Please refer to the following links for example python scripts for different models.


* `Decision-Trees (DT) <https://github.com/pedrojrv/ML_Nuclear_Data/blob/master/ML_EXFOR_neutrons/2_DT/dt.py>`_
* `K-Nearest-Neighbor (KNN) <https://github.com/pedrojrv/ML_Nuclear_Data/blob/master/ML_EXFOR_neutrons/1_KNN/knn.py>`_
* `XGBoost (Gradient Boosting Machines) <https://github.com/pedrojrv/ML_Nuclear_Data/blob/master/ML_EXFOR_neutrons/3_XGB/xgb.py>`_
* `Neural Networks <https://github.com/pedrojrv/ML_Nuclear_Data/tree/master/ML_EXFOR_neutrons/4_NN>`_ 



Processing Data for Monte Carlo
===============================

This section explains the philosophy behind the processing utilities and provides tutorial notebooks on how to use them. While it is possible to use 
raw predictions in benchmark calculations we have found that a hybrid aproach usually outperforms both pure ENDF- or ML-based solutions. 

.. toctree::
    :maxdepth: 2

    processing-datasets

Validating using Benchmarks
===========================

After processing your data, the final step is to validate the models using benchmark calculations. It is recommended that a model is 
tested using not one but multiple benchmarks. 

.. toctree::
    :maxdepth: 2

    validating-ml-models
